{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168fc75d",
   "metadata": {},
   "source": [
    "# üöÄ Multithreaded ResNet-18 Optimizer\n",
    "\n",
    "**Maximum Performance Animal Classifier**\n",
    "\n",
    "This notebook implements a highly optimized ResNet-18 model with:\n",
    "- ‚ö° **Multithreading** for data loading and processing\n",
    "- üî• **Maximum CPU/GPU utilization**\n",
    "- üéØ **Best accuracy optimization techniques**\n",
    "- üìä **Phase 1 & Phase 2 submissions**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1ef79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíª Local Jupyter detected\n",
      "üìÅ Base path: ..\n",
      "üñ•Ô∏è  Available CPU cores: 14\n",
      "üçé macOS detected - using conservative worker count\n",
      "‚ö° Using 2 workers for data loading\n",
      "üçé Metal Performance Shaders (MPS) available\n",
      "üîß Set multiprocessing start method to 'spawn' for macOS\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Environment Detection & Multithreading Setup\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "# Environment Detection\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Google Colab detected\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Local Jupyter detected\")\n",
    "\n",
    "# Base path configuration (adjusted for Multithreaded_approach folder)\n",
    "BASE_PATH = '/content' if IN_COLAB else '..'\n",
    "print(f\"üìÅ Base path: {BASE_PATH}\")\n",
    "\n",
    "# CPU/GPU Detection and Optimization\n",
    "cpu_count = mp.cpu_count()\n",
    "print(f\"üñ•Ô∏è  Available CPU cores: {cpu_count}\")\n",
    "\n",
    "# Set optimal number of workers based on platform\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    # Reduce workers on macOS to avoid multiprocessing issues\n",
    "    NUM_WORKERS = min(2, cpu_count // 2)  # Conservative for macOS\n",
    "    print(f\"üçé macOS detected - using conservative worker count\")\n",
    "elif IN_COLAB:\n",
    "    NUM_WORKERS = min(2, cpu_count)  # Conservative for Colab\n",
    "else:\n",
    "    NUM_WORKERS = min(cpu_count, 8)  # Cap at 8 for other platforms\n",
    "\n",
    "print(f\"‚ö° Using {NUM_WORKERS} workers for data loading\")\n",
    "\n",
    "# Torch optimization settings\n",
    "torch.set_num_threads(cpu_count)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• CUDA available - {torch.cuda.device_count()} GPUs\")\n",
    "    # Enable cudnn benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"üçé Metal Performance Shaders (MPS) available\")\n",
    "else:\n",
    "    print(\"üí™ Using optimized CPU with maximum threads\")\n",
    "\n",
    "# Set multiprocessing start method for macOS compatibility\n",
    "if platform.system() == 'Darwin' and not IN_COLAB:\n",
    "    try:\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "        print(\"üîß Set multiprocessing start method to 'spawn' for macOS\")\n",
    "    except RuntimeError:\n",
    "        print(\"‚ö†Ô∏è Multiprocessing start method already set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b042160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Installing optimized packages...\n",
      "‚úÖ torch\n",
      "‚úÖ torchvision\n",
      "‚úÖ torchaudio\n",
      "‚úÖ pandas\n",
      "‚úÖ numpy\n",
      "‚úÖ pillow\n",
      "‚úÖ scikit-learn\n",
      "‚úÖ tqdm\n",
      "‚úÖ requests\n",
      "‚úÖ matplotlib\n",
      "‚úÖ seaborn\n",
      "‚úÖ albumentations\n",
      "‚úÖ timm\n",
      "‚úÖ tensorboard\n",
      "üéØ Optimized for 14 CPU threads\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Enhanced Package Installation with Performance Optimization\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def install_packages():\n",
    "    # Core packages with performance optimizations\n",
    "    pkgs = [\n",
    "        'torch', 'torchvision', 'torchaudio',  # Latest PyTorch with optimizations\n",
    "        'pandas', 'numpy', 'pillow', 'scikit-learn', \n",
    "        'tqdm', 'requests', 'matplotlib', 'seaborn',\n",
    "        'albumentations',  # Advanced augmentations\n",
    "        'timm',  # State-of-the-art models\n",
    "        'tensorboard',  # Training visualization\n",
    "    ]\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        pkgs.extend(['gdown', 'fastai'])  # Additional Colab optimizations\n",
    "    \n",
    "    print(\"üîß Installing optimized packages...\")\n",
    "    for pkg in pkgs:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', pkg, '--upgrade'], \n",
    "                         check=True, capture_output=True)\n",
    "            print(f\"‚úÖ {pkg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to install {pkg}: {e}\")\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# Set environment variables for maximum performance\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count)\n",
    "os.environ['MKL_NUM_THREADS'] = str(cpu_count)\n",
    "print(f\"üéØ Optimized for {cpu_count} CPU threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Advanced Data Download & Organization\n",
    "if IN_COLAB:\n",
    "    import gdown\n",
    "    print(\"üì• Downloading datasets in parallel...\")\n",
    "    \n",
    "    # Parallel download using threading\n",
    "    import threading\n",
    "    \n",
    "    def download_and_extract(url, filename, extract_to):\n",
    "        try:\n",
    "            gdown.download(url, f'{BASE_PATH}/{filename}', quiet=False)\n",
    "            os.system(f'cd {BASE_PATH} && unzip -q {filename}')\n",
    "            os.system(f'rm -rf {BASE_PATH}/__MACOSX')\n",
    "            os.system(f'mv {BASE_PATH}/{extract_to}/* {BASE_PATH}/')\n",
    "            os.system(f'rm -rf {BASE_PATH}/{extract_to} {BASE_PATH}/{filename}')\n",
    "            print(f\"‚úÖ {filename} processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {filename}: {e}\")\n",
    "    \n",
    "    # Download both datasets in parallel\n",
    "    thread1 = threading.Thread(target=download_and_extract, \n",
    "                              args=('https://drive.google.com/uc?id=18MA0qKg1rqP92HApr_Fjck7Zo4Bwdqdu', \n",
    "                                   'HV-AI-2025.zip', 'HV-AI-2025'))\n",
    "    thread2 = threading.Thread(target=download_and_extract,\n",
    "                              args=('https://drive.google.com/uc?id=1aszVlQFQOwJTy9tt79s7x87VJyYw-Sxy',\n",
    "                                   'HV-AI-2025-Test.zip', 'HV-AI-2025-Test'))\n",
    "    \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    print(\"üéâ All datasets downloaded and organized!\")\n",
    "else:\n",
    "    print(\"üíª Assuming data is present in parent directory structure\")\n",
    "    print(f\"   - Labeled data: {BASE_PATH}/labeled_data/\")\n",
    "    print(f\"   - Unlabeled data: {BASE_PATH}/unlabeled_data/\")\n",
    "    print(f\"   - Test images: {BASE_PATH}/test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad45d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hariharan/Hiring/HyperVerge/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üçé Using Metal Performance Shaders (MPS)\n",
      "‚ö° PyTorch version: 2.7.1\n",
      "üéØ Device: mps\n",
      "üî• Mixed Precision: False\n"
     ]
    }
   ],
   "source": [
    "# üî• Enhanced Imports & Device Optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Enhanced data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "\n",
    "# Analysis and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Device optimization with detailed info\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'üöÄ Using CUDA - {gpu_name}')\n",
    "    print(f'üìä GPU Memory: {gpu_memory:.1f} GB')\n",
    "    # Enable mixed precision for faster training\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    scaler = GradScaler()\n",
    "    USE_AMP = True\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('üçé Using Metal Performance Shaders (MPS)')\n",
    "    USE_AMP = False  # MPS doesn't support AMP yet\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('üí™ Using optimized CPU')\n",
    "    USE_AMP = False\n",
    "\n",
    "print(f\"‚ö° PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üî• Mixed Precision: {USE_AMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6fc75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Loading and analyzing dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BASE_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Loading and analyzing dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load data with enhanced analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mBASE_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/labeled_data/labeled_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìà Dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müè∑Ô∏è  Columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'BASE_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# üìä Advanced Data Analysis & Preprocessing\n",
    "print(\"üîç Loading and analyzing dataset...\")\n",
    "\n",
    "# Load data with enhanced analysis\n",
    "df = pd.read_csv(f'{BASE_PATH}/labeled_data/labeled_data.csv')\n",
    "print(f\"üìà Dataset shape: {df.shape}\")\n",
    "print(f\"üè∑Ô∏è  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Enhanced label encoding and analysis\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nüéØ Number of classes: {num_classes}\")\n",
    "print(\"üìä Class distribution:\")\n",
    "class_counts = df['label'].value_counts()\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"   {label}: {count} samples\")\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "class_counts_array = np.bincount(df['encoded_label'])\n",
    "class_weights = 1.0 / class_counts_array\n",
    "class_weights = class_weights / class_weights.sum() * num_classes\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class weights calculated for balanced training\")\n",
    "print(\"üìà Data analysis complete!\")\n",
    "\n",
    "# Memory optimization\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d486c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Setting up advanced augmentations...\n",
      "‚úÖ Advanced augmentations configured!\n"
     ]
    }
   ],
   "source": [
    "# üé® Advanced Augmentations with Albumentations\n",
    "print(\"üé® Setting up advanced augmentations...\")\n",
    "\n",
    "# Heavy training augmentations for maximum generalization\n",
    "train_transforms = A.Compose([\n",
    "    A.Resize(256, 256),\n",
    "    A.RandomCrop(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.2),\n",
    "    A.RandomRotate90(p=0.3),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.3),\n",
    "    A.GaussNoise(var_limit=0.01, p=0.3),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation transforms with Test Time Augmentation options\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# TTA transforms for inference\n",
    "tta_transforms = [\n",
    "    A.Compose([A.Resize(224, 224), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),\n",
    "    A.Compose([A.Resize(224, 224), A.HorizontalFlip(p=1.0), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),\n",
    "    A.Compose([A.Resize(256, 256), A.CenterCrop(224, 224), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),\n",
    "    A.Compose([A.Resize(224, 224), A.VerticalFlip(p=1.0), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()]),\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Advanced augmentations configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b99d3792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating optimized train/validation splits...\n",
      "üìä Dataset: 623/623 valid images\n",
      "üìä Dataset: 156/156 valid images\n",
      "üéØ Optimal batch size: 16\n",
      "‚úÖ Data loaders created with 2 workers\n",
      "üîß Train loader: 39 batches\n",
      "üîß Val loader: 10 batches\n"
     ]
    }
   ],
   "source": [
    "# üß† Optimized Dataset with Multithreading\n",
    "class OptimizedAnimalDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_dir, transform=None, cache_images=False):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.cache_images = cache_images\n",
    "        self.image_cache = {} if cache_images else None\n",
    "        \n",
    "        # Pre-validate image paths\n",
    "        self.valid_indices = []\n",
    "        for idx in range(len(self.dataframe)):\n",
    "            img_name = self.dataframe.iloc[idx]['img_name']\n",
    "            img_path = os.path.join(self.images_dir, img_name)\n",
    "            if os.path.exists(img_path):\n",
    "                self.valid_indices.append(idx)\n",
    "        \n",
    "        print(f\"üìä Dataset: {len(self.valid_indices)}/{len(self.dataframe)} valid images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.valid_indices[idx]\n",
    "        img_name = self.dataframe.iloc[real_idx]['img_name']\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        # Use cached image if available\n",
    "        if self.cache_images and img_path in self.image_cache:\n",
    "            image = self.image_cache[img_path].copy()\n",
    "        else:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                if self.cache_images:\n",
    "                    self.image_cache[img_path] = image.copy()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                # Return a black image as fallback\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        label = self.dataframe.iloc[real_idx]['encoded_label']\n",
    "        \n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, A.Compose):\n",
    "                # Albumentations transform\n",
    "                image_np = np.array(image)\n",
    "                transformed = self.transform(image=image_np)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                # Torchvision transform\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Enhanced data splitting with stratification\n",
    "print(\"üîÑ Creating optimized train/validation splits...\")\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "# Create optimized datasets\n",
    "train_dataset = OptimizedAnimalDataset(\n",
    "    train_df, f'{BASE_PATH}/labeled_data/images', \n",
    "    train_transforms, cache_images=False  # Don't cache training images due to augmentations\n",
    ")\n",
    "\n",
    "val_dataset = OptimizedAnimalDataset(\n",
    "    val_df, f'{BASE_PATH}/labeled_data/images', \n",
    "    val_transforms, cache_images=True  # Cache validation images for speed\n",
    ")\n",
    "\n",
    "# Calculate optimal batch size based on available memory and platform\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    optimal_batch_size = min(64, max(16, int(gpu_memory_gb * 8)))  # Heuristic\n",
    "elif platform.system() == 'Darwin':  # macOS\n",
    "    optimal_batch_size = 16  # Conservative for macOS\n",
    "else:\n",
    "    optimal_batch_size = min(32, NUM_WORKERS * 4)\n",
    "\n",
    "print(f\"üéØ Optimal batch size: {optimal_batch_size}\")\n",
    "\n",
    "# Create optimized data loaders with conservative settings for stability\n",
    "dataloader_kwargs = {\n",
    "    'batch_size': optimal_batch_size,\n",
    "    'pin_memory': torch.cuda.is_available(),\n",
    "}\n",
    "\n",
    "# Add multiprocessing settings only if we have workers\n",
    "if NUM_WORKERS > 0:\n",
    "    dataloader_kwargs.update({\n",
    "        'num_workers': NUM_WORKERS,\n",
    "        'persistent_workers': True,\n",
    "        'prefetch_factor': 2,\n",
    "    })\n",
    "else:\n",
    "    dataloader_kwargs['num_workers'] = 0\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    **dataloader_kwargs\n",
    ")\n",
    "\n",
    "# Use fewer workers for validation to reduce memory pressure\n",
    "val_dataloader_kwargs = dataloader_kwargs.copy()\n",
    "if NUM_WORKERS > 0:\n",
    "    val_dataloader_kwargs['num_workers'] = max(1, NUM_WORKERS//2)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    shuffle=False,\n",
    "    **val_dataloader_kwargs\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaders created with {NUM_WORKERS} workers\")\n",
    "print(f\"üîß Train loader: {len(train_loader)} batches\")\n",
    "print(f\"üîß Val loader: {len(val_loader)} batches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0e4d0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating enhanced ResNet-18 model...\n",
      "üçé Skipping torch.compile on macOS for compatibility\n",
      "üéØ Model created with 11,310,922 parameters\n",
      "üîß Testing model architecture...\n",
      "‚úÖ Model test successful - Input: torch.Size([2, 3, 224, 224]), Output: torch.Size([2, 10])\n",
      "‚úÖ Output shape correct: torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "# üèóÔ∏è Enhanced ResNet-18 with Optimizations (Fixed Architecture)\n",
    "class EnhancedResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, dropout_rate=0.5):\n",
    "        super(EnhancedResNet18, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet-18\n",
    "        self.backbone = models.resnet18(weights='IMAGENET1K_V1' if pretrained else None)\n",
    "        \n",
    "        # Get the number of features before the final layer\n",
    "        num_features = self.backbone.fc.in_features\n",
    "        \n",
    "        # Remove the final classification layer\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        \n",
    "        # Enhanced classifier head with proper architecture\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(num_features, num_features // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(num_features // 2),\n",
    "            nn.Dropout(dropout_rate / 2),\n",
    "            nn.Linear(num_features // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward through backbone (includes global average pooling)\n",
    "        features = self.backbone(x)\n",
    "        # features should now be (batch_size, 512)\n",
    "        return self.classifier(features)\n",
    "\n",
    "# Create enhanced model\n",
    "print(\"üèóÔ∏è Creating enhanced ResNet-18 model...\")\n",
    "model = EnhancedResNet18(num_classes=num_classes, pretrained=True, dropout_rate=0.3)\n",
    "\n",
    "# Move to device and optimize\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"üî• Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Skip model compilation for macOS to avoid compatibility issues\n",
    "if platform.system() != 'Darwin':\n",
    "    # Compile model for PyTorch 2.0+ optimization (non-macOS only)\n",
    "    if hasattr(torch, 'compile'):\n",
    "        try:\n",
    "            model = torch.compile(model)\n",
    "            print(\"‚ö° Model compiled with PyTorch 2.0 optimization\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è PyTorch compile not available, using standard model\")\n",
    "else:\n",
    "    print(\"üçé Skipping torch.compile on macOS for compatibility\")\n",
    "\n",
    "print(f\"üéØ Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test the model with a sample input to verify architecture\n",
    "print(\"üîß Testing model architecture...\")\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.randn(2, 3, 224, 224).to(device)\n",
    "    sample_output = model(sample_input)\n",
    "    print(f\"‚úÖ Model test successful - Input: {sample_input.shape}, Output: {sample_output.shape}\")\n",
    "    expected_shape = (2, num_classes)\n",
    "    if sample_output.shape == expected_shape:\n",
    "        print(f\"‚úÖ Output shape correct: {sample_output.shape}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Output shape mismatch: expected {expected_shape}, got {sample_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "778d003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Setting up advanced optimizers and loss functions...\n",
      "‚úÖ Advanced optimizers and loss functions configured!\n",
      "üìä Using class-weighted focal loss with label smoothing\n",
      "‚ö° AdamW optimizer with cosine annealing scheduler\n"
     ]
    }
   ],
   "source": [
    "# üî• Advanced Optimizers & Loss Functions\n",
    "print(\"üî• Setting up advanced optimizers and loss functions...\")\n",
    "\n",
    "# Enhanced loss function with class weights and label smoothing\n",
    "class EnhancedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, weight=None, label_smoothing=0.1, focal_alpha=0.25, focal_gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.focal_alpha = focal_alpha\n",
    "        self.focal_gamma = focal_gamma\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # Standard cross entropy with label smoothing\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, \n",
    "                                 label_smoothing=self.label_smoothing, reduction='none')\n",
    "        \n",
    "        # Add focal loss component for hard examples\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_weight = self.focal_alpha * (1 - pt) ** self.focal_gamma\n",
    "        focal_loss = focal_weight * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Create enhanced loss function\n",
    "criterion = EnhancedCrossEntropyLoss(\n",
    "    weight=class_weights_tensor, \n",
    "    label_smoothing=0.1,\n",
    "    focal_alpha=0.25,\n",
    "    focal_gamma=2.0\n",
    ")\n",
    "\n",
    "# Advanced optimizer with weight decay and gradient clipping\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=3e-4,  # Lower initial learning rate\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=20,  # Total epochs\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Backup scheduler for plateau detection\n",
    "plateau_scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    factor=0.5, \n",
    "    patience=3, \n",
    "    min_lr=1e-7\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Advanced optimizers and loss functions configured!\")\n",
    "print(f\"üìä Using class-weighted focal loss with label smoothing\")\n",
    "print(f\"‚ö° AdamW optimizer with cosine annealing scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57fa0ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Creating single-threaded data loaders for macOS compatibility...\n",
      "üî• Starting optimized training with stable single-threaded loaders...\n",
      "‚ö° Platform: Darwin\n",
      "üîß Workers: 0 (single-threaded for stability)\n",
      "üì¶ Batch size: 16\n",
      "üçé Using Metal Performance Shaders for acceleration\n",
      "üöÄ Starting optimized training with all enhancements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.61it/s, Loss=0.4719, Acc=20.71%, LR=3.00e-04]\n",
      "Epoch 1/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:05<00:00,  7.61it/s, Loss=0.4719, Acc=20.71%, LR=3.00e-04]\n",
      "Epoch 1/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 20.06it/s, Loss=0.2373, Acc=49.36%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "  Train Loss: 0.4719, Train Acc: 20.71%\n",
      "  Val Loss: 0.2610, Val Acc: 49.36%\n",
      "  Time: 5.62s, LR: 2.98e-04\n",
      "  ‚úÖ New best validation accuracy: 49.36%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.84it/s, Loss=0.3943, Acc=26.81%, LR=2.98e-04]\n",
      "Epoch 2/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.84it/s, Loss=0.3943, Acc=26.81%, LR=2.98e-04]\n",
      "Epoch 2/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 51.05it/s, Loss=0.3291, Acc=51.28%]\n",
      "Epoch 2/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 51.05it/s, Loss=0.3291, Acc=51.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "  Train Loss: 0.3943, Train Acc: 26.81%\n",
      "  Val Loss: 0.3620, Val Acc: 51.28%\n",
      "  Time: 3.24s, LR: 2.93e-04\n",
      "  ‚úÖ New best validation accuracy: 51.28%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.32it/s, Loss=0.3773, Acc=30.66%, LR=2.93e-04]\n",
      "Epoch 3/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.32it/s, Loss=0.3773, Acc=30.66%, LR=2.93e-04]\n",
      "Epoch 3/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 49.91it/s, Loss=0.3041, Acc=41.67%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "  Train Loss: 0.3773, Train Acc: 30.66%\n",
      "  Val Loss: 0.3345, Val Acc: 41.67%\n",
      "  Time: 3.13s, LR: 2.84e-04\n",
      "  ‚è≥ Patience: 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.10it/s, Loss=0.3765, Acc=29.86%, LR=2.84e-04]\n",
      "Epoch 4/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.10it/s, Loss=0.3765, Acc=29.86%, LR=2.84e-04]\n",
      "Epoch 4/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.28it/s, Loss=0.2999, Acc=52.56%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "  Train Loss: 0.3765, Train Acc: 29.86%\n",
      "  Val Loss: 0.3299, Val Acc: 52.56%\n",
      "  Time: 3.19s, LR: 2.71e-04\n",
      "  ‚úÖ New best validation accuracy: 52.56%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.96it/s, Loss=0.3383, Acc=33.55%, LR=2.71e-04]\n",
      "Epoch 5/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.96it/s, Loss=0.3383, Acc=33.55%, LR=2.71e-04]\n",
      "Epoch 5/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.34it/s, Loss=0.2279, Acc=58.33%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "  Train Loss: 0.3383, Train Acc: 33.55%\n",
      "  Val Loss: 0.2507, Val Acc: 58.33%\n",
      "  Time: 3.22s, LR: 2.56e-04\n",
      "  ‚úÖ New best validation accuracy: 58.33%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.93it/s, Loss=0.3269, Acc=37.72%, LR=2.56e-04]\n",
      "Epoch 6/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.93it/s, Loss=0.3269, Acc=37.72%, LR=2.56e-04]\n",
      "Epoch 6/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.14it/s, Loss=0.2442, Acc=58.33%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "  Train Loss: 0.3269, Train Acc: 37.72%\n",
      "  Val Loss: 0.2686, Val Acc: 58.33%\n",
      "  Time: 3.23s, LR: 2.38e-04\n",
      "  ‚è≥ Patience: 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.01it/s, Loss=0.2879, Acc=43.18%, LR=2.38e-04]\n",
      "Epoch 7/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.01it/s, Loss=0.2879, Acc=43.18%, LR=2.38e-04]\n",
      "Epoch 7/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.95it/s, Loss=0.2203, Acc=54.49%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "  Train Loss: 0.2879, Train Acc: 43.18%\n",
      "  Val Loss: 0.2424, Val Acc: 54.49%\n",
      "  Time: 3.21s, LR: 2.18e-04\n",
      "  ‚è≥ Patience: 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.91it/s, Loss=0.2712, Acc=41.89%, LR=2.18e-04]\n",
      "Epoch 8/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.91it/s, Loss=0.2712, Acc=41.89%, LR=2.18e-04]\n",
      "Epoch 8/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.82it/s, Loss=0.1809, Acc=66.03%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "  Train Loss: 0.2712, Train Acc: 41.89%\n",
      "  Val Loss: 0.1990, Val Acc: 66.03%\n",
      "  Time: 3.23s, LR: 1.97e-04\n",
      "  ‚úÖ New best validation accuracy: 66.03%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.11it/s, Loss=0.2725, Acc=44.94%, LR=1.97e-04]\n",
      "Epoch 9/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:02<00:00, 13.11it/s, Loss=0.2725, Acc=44.94%, LR=1.97e-04]\n",
      "Epoch 9/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.76it/s, Loss=0.1795, Acc=58.97%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "  Train Loss: 0.2725, Train Acc: 44.94%\n",
      "  Val Loss: 0.1974, Val Acc: 58.97%\n",
      "  Time: 3.18s, LR: 1.74e-04\n",
      "  ‚è≥ Patience: 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.98it/s, Loss=0.2373, Acc=50.08%, LR=1.74e-04]\n",
      "Epoch 10/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.98it/s, Loss=0.2373, Acc=50.08%, LR=1.74e-04]\n",
      "Epoch 10/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.00it/s, Loss=0.1543, Acc=71.79%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:\n",
      "  Train Loss: 0.2373, Train Acc: 50.08%\n",
      "  Val Loss: 0.1697, Val Acc: 71.79%\n",
      "  Time: 3.21s, LR: 1.50e-04\n",
      "  ‚úÖ New best validation accuracy: 71.79%\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.86it/s, Loss=0.2116, Acc=56.18%, LR=1.50e-04]\n",
      "Epoch 11/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.86it/s, Loss=0.2116, Acc=56.18%, LR=1.50e-04]\n",
      "Epoch 11/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.84it/s, Loss=0.1655, Acc=71.79%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:\n",
      "  Train Loss: 0.2116, Train Acc: 56.18%\n",
      "  Val Loss: 0.1820, Val Acc: 71.79%\n",
      "  Time: 3.24s, LR: 1.27e-04\n",
      "  ‚è≥ Patience: 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.95it/s, Loss=0.2163, Acc=55.38%, LR=1.27e-04]\n",
      "Epoch 12/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.95it/s, Loss=0.2163, Acc=55.38%, LR=1.27e-04]\n",
      "Epoch 12/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.48it/s, Loss=0.1481, Acc=71.15%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:\n",
      "  Train Loss: 0.2163, Train Acc: 55.38%\n",
      "  Val Loss: 0.1629, Val Acc: 71.15%\n",
      "  Time: 3.22s, LR: 1.04e-04\n",
      "  ‚è≥ Patience: 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.60it/s, Loss=0.1821, Acc=61.48%, LR=1.04e-04]\n",
      "Epoch 13/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.60it/s, Loss=0.1821, Acc=61.48%, LR=1.04e-04]\n",
      "Epoch 13/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.16it/s, Loss=0.1486, Acc=71.15%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:\n",
      "  Train Loss: 0.1821, Train Acc: 61.48%\n",
      "  Val Loss: 0.1635, Val Acc: 71.15%\n",
      "  Time: 3.31s, LR: 8.26e-05\n",
      "  ‚è≥ Patience: 3/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.99it/s, Loss=0.1979, Acc=60.51%, LR=8.26e-05]\n",
      "Epoch 14/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.99it/s, Loss=0.1979, Acc=60.51%, LR=8.26e-05]\n",
      "Epoch 14/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 47.74it/s, Loss=0.1576, Acc=70.51%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:\n",
      "  Train Loss: 0.1979, Train Acc: 60.51%\n",
      "  Val Loss: 0.1734, Val Acc: 70.51%\n",
      "  Time: 3.21s, LR: 3.13e-05\n",
      "  ‚è≥ Patience: 4/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.92it/s, Loss=0.1690, Acc=63.72%, LR=3.13e-05]\n",
      "Epoch 15/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.92it/s, Loss=0.1690, Acc=63.72%, LR=3.13e-05]\n",
      "Epoch 15/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 46.49it/s, Loss=0.1395, Acc=75.64%]\n",
      "Epoch 15/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 46.49it/s, Loss=0.1395, Acc=75.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:\n",
      "  Train Loss: 0.1690, Train Acc: 63.72%\n",
      "  Val Loss: 0.1535, Val Acc: 75.64%\n",
      "  Time: 3.24s, LR: 2.25e-05\n",
      "  ‚úÖ New best validation accuracy: 75.64%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 13.00it/s, Loss=0.1683, Acc=64.69%, LR=2.25e-05]\n",
      "Epoch 16/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 13.00it/s, Loss=0.1683, Acc=64.69%, LR=2.25e-05]\n",
      "Epoch 16/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 48.53it/s, Loss=0.1367, Acc=76.28%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:\n",
      "  Train Loss: 0.1683, Train Acc: 64.69%\n",
      "  Val Loss: 0.1504, Val Acc: 76.28%\n",
      "  Time: 3.21s, LR: 1.50e-05\n",
      "  ‚úÖ New best validation accuracy: 76.28%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.23it/s, Loss=0.1587, Acc=67.58%, LR=1.50e-05]\n",
      "Epoch 17/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.23it/s, Loss=0.1587, Acc=67.58%, LR=1.50e-05]\n",
      "Epoch 17/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.72it/s, Loss=0.1332, Acc=78.85%]\n",
      "Epoch 17/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.72it/s, Loss=0.1332, Acc=78.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:\n",
      "  Train Loss: 0.1587, Train Acc: 67.58%\n",
      "  Val Loss: 0.1465, Val Acc: 78.85%\n",
      "  Time: 3.41s, LR: 9.02e-06\n",
      "  ‚úÖ New best validation accuracy: 78.85%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.65it/s, Loss=0.1603, Acc=63.88%, LR=9.02e-06]\n",
      "Epoch 18/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.65it/s, Loss=0.1603, Acc=63.88%, LR=9.02e-06]\n",
      "Epoch 18/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.10it/s, Loss=0.1366, Acc=76.92%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:\n",
      "  Train Loss: 0.1603, Train Acc: 63.88%\n",
      "  Val Loss: 0.1503, Val Acc: 76.92%\n",
      "  Time: 3.31s, LR: 4.60e-06\n",
      "  ‚è≥ Patience: 1/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.71it/s, Loss=0.1544, Acc=67.26%, LR=4.60e-06]\n",
      "Epoch 19/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.71it/s, Loss=0.1544, Acc=67.26%, LR=4.60e-06]\n",
      "Epoch 19/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 46.32it/s, Loss=0.1303, Acc=78.21%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:\n",
      "  Train Loss: 0.1544, Train Acc: 67.26%\n",
      "  Val Loss: 0.1434, Val Acc: 78.21%\n",
      "  Time: 3.29s, LR: 1.91e-06\n",
      "  ‚è≥ Patience: 2/5\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.56it/s, Loss=0.1655, Acc=65.81%, LR=1.91e-06]\n",
      "Epoch 20/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 39/39 [00:03<00:00, 12.56it/s, Loss=0.1655, Acc=65.81%, LR=1.91e-06]\n",
      "Epoch 20/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.12it/s, Loss=0.1288, Acc=80.13%]\n",
      "Epoch 20/20 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 45.12it/s, Loss=0.1288, Acc=80.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:\n",
      "  Train Loss: 0.1655, Train Acc: 65.81%\n",
      "  Val Loss: 0.1417, Val Acc: 80.13%\n",
      "  Time: 3.33s, LR: 1.00e-06\n",
      "  ‚úÖ New best validation accuracy: 80.13%\n",
      "------------------------------------------------------------\n",
      "üéØ Training completed!\n",
      "üìä Best Validation Accuracy: 80.13%\n",
      "üìâ Best Validation Loss: 0.1417\n",
      "------------------------------------------------------------\n",
      "üéØ Training completed!\n",
      "üìä Best Validation Accuracy: 80.13%\n",
      "üìâ Best Validation Loss: 0.1417\n"
     ]
    }
   ],
   "source": [
    "# ‚ö° Optimized Training Loop (Single-threaded for macOS compatibility)\n",
    "def train_model_optimized(model, train_loader, val_loader, criterion, optimizer, \n",
    "                         scheduler, device, epochs=20, patience=5):\n",
    "    print(\"üöÄ Starting optimized training with all enhancements...\")\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_pbar):\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            \n",
    "            # Mixed precision training\n",
    "            if USE_AMP:\n",
    "                with autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                optimizer.zero_grad()\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard training\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            current_acc = 100 * correct / total\n",
    "            train_pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'Acc': f'{current_acc:.2f}%',\n",
    "                'LR': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                \n",
    "                if USE_AMP:\n",
    "                    with autocast():\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'Loss': f'{val_loss/(len(val_pbar)+1):.4f}',\n",
    "                    'Acc': f'{100*val_correct/val_total:.2f}%'\n",
    "                })\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step()\n",
    "        plateau_scheduler.step(val_acc)\n",
    "        \n",
    "        # Calculate epoch time\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        print(f'  Time: {epoch_time:.2f}s, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save model\n",
    "            if hasattr(model, 'module'):  # DataParallel\n",
    "                torch.save(model.module.state_dict(), f'{BASE_PATH}/best_multithreaded_resnet18.pth')\n",
    "            else:\n",
    "                torch.save(model.state_dict(), f'{BASE_PATH}/best_multithreaded_resnet18.pth')\n",
    "            \n",
    "            print(f'  ‚úÖ New best validation accuracy: {best_acc:.2f}%')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f'  ‚è≥ Patience: {patience_counter}/{patience}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'üõë Early stopping triggered after {epoch+1} epochs')\n",
    "            break\n",
    "        \n",
    "        # Memory cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        print('-' * 60)\n",
    "    \n",
    "    print(f'üéØ Training completed!')\n",
    "    print(f'üìä Best Validation Accuracy: {best_acc:.2f}%')\n",
    "    print(f'üìâ Best Validation Loss: {best_loss:.4f}')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accs': train_accs,\n",
    "        'val_accs': val_accs,\n",
    "        'best_acc': best_acc,\n",
    "        'best_loss': best_loss\n",
    "    }\n",
    "\n",
    "# Create single-threaded data loaders for macOS compatibility\n",
    "print(\"üîß Creating single-threaded data loaders for macOS compatibility...\")\n",
    "train_loader_stable = DataLoader(\n",
    "    train_dataset, batch_size=optimal_batch_size, shuffle=True, num_workers=0, \n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "val_loader_stable = DataLoader(\n",
    "    val_dataset, batch_size=optimal_batch_size, shuffle=False, num_workers=0, \n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Start optimized training with stable loaders\n",
    "print(\"üî• Starting optimized training with stable single-threaded loaders...\")\n",
    "print(f\"‚ö° Platform: {platform.system()}\")\n",
    "print(f\"üîß Workers: 0 (single-threaded for stability)\")\n",
    "print(f\"üì¶ Batch size: {optimal_batch_size}\")\n",
    "print(f\"üçé Using Metal Performance Shaders for acceleration\")\n",
    "\n",
    "model, training_history = train_model_optimized(\n",
    "    model, train_loader_stable, val_loader_stable, criterion, optimizer, \n",
    "    scheduler, device, epochs=20, patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81f6d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading best model for testing...\n",
      "üìä Testing model performance...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 28.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Overall Test Accuracy: 80.13%\n",
      "\n",
      "üìä Per-class Accuracy:\n",
      "   cane: 68.97% (20/29)\n",
      "   cavallo: 81.25% (13/16)\n",
      "   elefante: 87.50% (7/8)\n",
      "   farfalla: 84.62% (11/13)\n",
      "   gallina: 77.78% (14/18)\n",
      "   gatto: 70.00% (7/10)\n",
      "   mucca: 72.73% (8/11)\n",
      "   pecora: 81.82% (9/11)\n",
      "   ragno: 96.55% (28/29)\n",
      "   scoiattolo: 72.73% (8/11)\n",
      "\n",
      "üìà Macro F1-Score: 0.7839\n",
      "üìà Weighted F1-Score: 0.8085\n",
      "\n",
      "üéâ Final Test Results:\n",
      "üéØ Test Accuracy: 80.13%\n",
      "üìä Average Class Accuracy: 79.39%\n",
      "üîß Model successfully trained and tested on macOS!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# üìä Simple Performance Testing (Single-threaded for macOS)\n",
    "def test_model_simple(model, val_loader, device):\n",
    "    print(\"üìä Testing model performance...\")\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Standard inference without TTA for simplicity\n",
    "            outputs = model(images)\n",
    "            predicted = outputs.argmax(dim=1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Store predictions for analysis\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Per-class accuracy\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(labels.size(0)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Overall accuracy\n",
    "    overall_acc = 100 * correct / total\n",
    "    print(f'üéØ Overall Test Accuracy: {overall_acc:.2f}%')\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print('\\nüìä Per-class Accuracy:')\n",
    "    class_accuracies = []\n",
    "    for i in range(num_classes):\n",
    "        class_name = label_encoder.inverse_transform([i])[0]\n",
    "        if class_total[i] > 0:\n",
    "            acc = 100 * class_correct[i] / class_total[i]\n",
    "            class_accuracies.append(acc)\n",
    "            print(f'   {class_name}: {acc:.2f}% ({int(class_correct[i])}/{int(class_total[i])})')\n",
    "        else:\n",
    "            class_accuracies.append(0.0)\n",
    "    \n",
    "    # Classification report\n",
    "    try:\n",
    "        from sklearn.metrics import classification_report\n",
    "        report = classification_report(all_labels, all_predictions, \n",
    "                                     target_names=label_encoder.classes_, \n",
    "                                     output_dict=True)\n",
    "        print(f'\\nüìà Macro F1-Score: {report[\"macro avg\"][\"f1-score\"]:.4f}')\n",
    "        print(f'üìà Weighted F1-Score: {report[\"weighted avg\"][\"f1-score\"]:.4f}')\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Could not generate classification report\")\n",
    "    \n",
    "    return overall_acc, class_accuracies, all_predictions, all_labels\n",
    "\n",
    "# Load best model and test\n",
    "print(\"üì• Loading best model for testing...\")\n",
    "if hasattr(model, 'module'):  # DataParallel\n",
    "    model.module.load_state_dict(torch.load(f'{BASE_PATH}/best_multithreaded_resnet18.pth', map_location=device))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{BASE_PATH}/best_multithreaded_resnet18.pth', map_location=device))\n",
    "\n",
    "# Test with simple evaluation (no TTA for macOS compatibility)\n",
    "test_acc, class_accs, preds, labels_test = test_model_simple(model, val_loader_stable, device)\n",
    "\n",
    "print(f\"\\nüéâ Final Test Results:\")\n",
    "print(f\"üéØ Test Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"üìä Average Class Accuracy: {np.mean(class_accs):.2f}%\")\n",
    "print(f\"üîß Model successfully trained and tested on macOS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b65a84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Phase 2 training with simplified pseudo-labeling...\n",
      "üìÅ Found 14800 unlabeled images\n",
      "üîç Generating pseudo labels with confidence >= 0.85...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pseudo-labeling: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 925/925 [00:34<00:00, 26.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 4312 pseudo labels\n",
      "üìä Combined dataset: 5091 samples (779 labeled + 4312 pseudo)\n",
      "üî• Starting Phase 2 fine-tuning...\n",
      "üöÄ Starting optimized training with all enhancements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:25<00:00, 10.57it/s, Loss=0.1485, Acc=74.19%, LR=1.00e-05]\n",
      "Epoch 1/5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:25<00:00, 10.57it/s, Loss=0.1485, Acc=74.19%, LR=1.00e-05]\n",
      "Epoch 1/5 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:01<00:00, 26.54it/s, Loss=0.0554, Acc=96.47%]\n",
      "Epoch 1/5 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:01<00:00, 26.54it/s, Loss=0.0554, Acc=96.47%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "  Train Loss: 0.1485, Train Acc: 74.19%\n",
      "  Val Loss: 0.0565, Val Acc: 96.47%\n",
      "  Time: 27.44s, LR: 9.05e-06\n",
      "  ‚úÖ New best validation accuracy: 96.47%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:23<00:00, 11.44it/s, Loss=0.1298, Acc=77.42%, LR=9.05e-06]\n",
      "Epoch 2/5 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271/271 [00:23<00:00, 11.44it/s, Loss=0.1298, Acc=77.42%, LR=9.05e-06]\n",
      "Epoch 2/5 [Val]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:01<00:00, 26.17it/s, Loss=0.0582, Acc=97.12%]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "  Train Loss: 0.1298, Train Acc: 77.42%\n",
      "  Val Loss: 0.0595, Val Acc: 97.12%\n",
      "  Time: 25.53s, LR: 6.58e-06\n",
      "  ‚úÖ New best validation accuracy: 97.12%\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]:  41%|‚ñà‚ñà‚ñà‚ñà      | 110/271 [00:09<00:14, 11.42it/s, Loss=0.1279, Acc=78.47%, LR=6.58e-06]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 188\u001b[39m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Run Phase 2 training\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m model_phase2 = \u001b[43mtrain_phase2_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.85\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 172\u001b[39m, in \u001b[36mtrain_phase2_simple\u001b[39m\u001b[34m(model, labeled_df, epochs, confidence_threshold)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Phase 2 training\u001b[39;00m\n\u001b[32m    171\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müî• Starting Phase 2 fine-tuning...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m model, phase2_history = \u001b[43mtrain_model_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_val_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase2_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase2_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m    176\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Save Phase 2 model\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mtrain_model_optimized\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, patience)\u001b[39m\n\u001b[32m     48\u001b[39m     loss.backward()\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     optimizer.step()\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hiring/HyperVerge/venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hiring/HyperVerge/venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:219\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    217\u001b[39m     parameters = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hiring/HyperVerge/venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Hiring/HyperVerge/venv/lib/python3.13/site-packages/torch/nn/utils/clip_grad.py:98\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     94\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     97\u001b[39m         norms.extend(\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m             [\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvector_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m device_tensors]\n\u001b[32m     99\u001b[39m         )\n\u001b[32m    101\u001b[39m total_norm = torch.linalg.vector_norm(\n\u001b[32m    102\u001b[39m     torch.stack([norm.to(first_device) \u001b[38;5;28;01mfor\u001b[39;00m norm \u001b[38;5;129;01min\u001b[39;00m norms]), norm_type\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite \u001b[38;5;129;01mand\u001b[39;00m torch.logical_or(total_norm.isnan(), total_norm.isinf()):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# üöÄ Phase 2 Training with Simplified Pseudo-Labeling (macOS Compatible)\n",
    "class SimplifiedUnlabeledDataset(Dataset):\n",
    "    def __init__(self, images_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in os.listdir(images_dir) \n",
    "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"üìÅ Found {len(self.image_files)} unlabeled images\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.images_dir, img_name)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color='black')\n",
    "        \n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, A.Compose):\n",
    "                image_np = np.array(image)\n",
    "                transformed = self.transform(image=image_np)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                image = self.transform(image)\n",
    "        \n",
    "        return image, img_name\n",
    "\n",
    "def generate_pseudo_labels_simple(model, unlabeled_loader, confidence_threshold=0.85):\n",
    "    print(f\"üîç Generating pseudo labels with confidence >= {confidence_threshold}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    pseudo_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, img_names in tqdm(unlabeled_loader, desc=\"Pseudo-labeling\"):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            max_probs, predicted = torch.max(probs, 1)\n",
    "            \n",
    "            for i, (prob, pred, img_name) in enumerate(zip(max_probs, predicted, img_names)):\n",
    "                confidence = prob.item()\n",
    "                if confidence >= confidence_threshold:\n",
    "                    pred_label = label_encoder.inverse_transform([pred.item()])[0]\n",
    "                    pseudo_labels.append({\n",
    "                        'img_name': img_name,\n",
    "                        'label': pred_label,\n",
    "                        'encoded_label': pred.item(),\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "    \n",
    "    return pd.DataFrame(pseudo_labels)\n",
    "\n",
    "def train_phase2_simple(model, labeled_df, epochs=5, confidence_threshold=0.85):\n",
    "    print(\"üöÄ Starting Phase 2 training with simplified pseudo-labeling...\")\n",
    "    \n",
    "    # Generate pseudo labels using single-threaded loader\n",
    "    unlabeled_dir = f'{BASE_PATH}/unlabeled_data/images'\n",
    "    unlabeled_dataset = SimplifiedUnlabeledDataset(unlabeled_dir, val_transforms)\n",
    "    unlabeled_loader = DataLoader(\n",
    "        unlabeled_dataset, \n",
    "        batch_size=optimal_batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0,  # Single-threaded for macOS\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    pseudo_df = generate_pseudo_labels_simple(model, unlabeled_loader, confidence_threshold)\n",
    "    print(f\"‚úÖ Generated {len(pseudo_df)} pseudo labels\")\n",
    "    \n",
    "    if len(pseudo_df) == 0:\n",
    "        print(\"‚ö†Ô∏è No pseudo labels generated, skipping Phase 2\")\n",
    "        return model\n",
    "    \n",
    "    # Combine datasets\n",
    "    combined_df = pd.concat([labeled_df, pseudo_df], ignore_index=True)\n",
    "    print(f\"üìä Combined dataset: {len(combined_df)} samples ({len(labeled_df)} labeled + {len(pseudo_df)} pseudo)\")\n",
    "    \n",
    "    # Split combined data\n",
    "    combined_train_df, combined_val_df = train_test_split(\n",
    "        combined_df, test_size=0.15, random_state=42, stratify=combined_df['label']\n",
    "    )\n",
    "    \n",
    "    # Combined dataset class\n",
    "    class CombinedDataset(Dataset):\n",
    "        def __init__(self, dataframe, labeled_dir, unlabeled_dir, transform=None):\n",
    "            self.dataframe = dataframe.reset_index(drop=True)\n",
    "            self.labeled_dir = labeled_dir\n",
    "            self.unlabeled_dir = unlabeled_dir\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.dataframe)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            row = self.dataframe.iloc[idx]\n",
    "            img_name = row['img_name']\n",
    "            \n",
    "            # Check directories\n",
    "            labeled_path = os.path.join(self.labeled_dir, img_name)\n",
    "            if os.path.exists(labeled_path):\n",
    "                img_path = labeled_path\n",
    "            else:\n",
    "                img_path = os.path.join(self.unlabeled_dir, img_name)\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                image = Image.new('RGB', (224, 224), color='black')\n",
    "            \n",
    "            label = row['encoded_label']\n",
    "            \n",
    "            if self.transform:\n",
    "                if isinstance(self.transform, A.Compose):\n",
    "                    image_np = np.array(image)\n",
    "                    transformed = self.transform(image=image_np)\n",
    "                    image = transformed['image']\n",
    "                else:\n",
    "                    image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "    \n",
    "    # Create combined datasets\n",
    "    combined_train_dataset = CombinedDataset(\n",
    "        combined_train_df,\n",
    "        f'{BASE_PATH}/labeled_data/images',\n",
    "        f'{BASE_PATH}/unlabeled_data/images',\n",
    "        train_transforms\n",
    "    )\n",
    "    \n",
    "    combined_val_dataset = CombinedDataset(\n",
    "        combined_val_df,\n",
    "        f'{BASE_PATH}/labeled_data/images',\n",
    "        f'{BASE_PATH}/unlabeled_data/images',\n",
    "        val_transforms\n",
    "    )\n",
    "    \n",
    "    # Create combined loaders (single-threaded)\n",
    "    combined_train_loader = DataLoader(\n",
    "        combined_train_dataset,\n",
    "        batch_size=optimal_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Single-threaded\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    combined_val_loader = DataLoader(\n",
    "        combined_val_dataset,\n",
    "        batch_size=optimal_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Single-threaded\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    # Phase 2 optimizer with lower learning rate\n",
    "    phase2_optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,  # Much lower learning rate for fine-tuning\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    phase2_scheduler = CosineAnnealingLR(phase2_optimizer, T_max=epochs, eta_min=1e-7)\n",
    "    \n",
    "    # Phase 2 training\n",
    "    print(\"üî• Starting Phase 2 fine-tuning...\")\n",
    "    model, phase2_history = train_model_optimized(\n",
    "        model, combined_train_loader, combined_val_loader,\n",
    "        criterion, phase2_optimizer, phase2_scheduler,\n",
    "        device, epochs=epochs, patience=3\n",
    "    )\n",
    "    \n",
    "    # Save Phase 2 model\n",
    "    if hasattr(model, 'module'):\n",
    "        torch.save(model.module.state_dict(), f'{BASE_PATH}/best_multithreaded_resnet18_phase2.pth')\n",
    "    else:\n",
    "        torch.save(model.state_dict(), f'{BASE_PATH}/best_multithreaded_resnet18_phase2.pth')\n",
    "    \n",
    "    print(\"‚úÖ Phase 2 training completed!\")\n",
    "    return model\n",
    "\n",
    "# Run Phase 2 training\n",
    "model_phase2 = train_phase2_simple(model, df, epochs=5, confidence_threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f3ebb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Generating Phase 1 predictions...\n",
      "üì• Loading best Phase 1 model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Load the best Phase 1 model\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müì• Loading best Phase 1 model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mmodel\u001b[49m, \u001b[33m'\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     59\u001b[39m     model.module.load_state_dict(torch.load(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/best_multithreaded_resnet18.pth\u001b[39m\u001b[33m'\u001b[39m, map_location=device))\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# üìà Simple Prediction Generation (macOS Compatible)\n",
    "def predict_and_save_simple(model, test_dir, label_encoder, output_csv):\n",
    "    print(f\"üîÆ Generating predictions for {output_csv}...\")\n",
    "    \n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    # Get all test images\n",
    "    test_files = [f for f in os.listdir(test_dir) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    test_files.sort()\n",
    "    \n",
    "    print(f\"üìÅ Found {len(test_files)} test images\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(test_files, desc=\"Predicting\"):\n",
    "            img_path = os.path.join(test_dir, fname)\n",
    "            \n",
    "            try:\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "                # Use most common class as fallback\n",
    "                pred_label = label_encoder.classes_[0]\n",
    "                results.append({'path': fname, 'predicted_label': pred_label})\n",
    "                continue\n",
    "            \n",
    "            # Standard prediction (no TTA for simplicity)\n",
    "            image_np = np.array(image)\n",
    "            transformed = val_transforms(image=image_np)\n",
    "            img_tensor = transformed['image'].unsqueeze(0).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(img_tensor)\n",
    "            pred_idx = output.argmax(1).item()\n",
    "            pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "            results.append({'path': fname, 'predicted_label': pred_label})\n",
    "    \n",
    "    # Save predictions\n",
    "    pred_df = pd.DataFrame(results)\n",
    "    pred_df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Saved {len(results)} predictions to {output_csv}\")\n",
    "    \n",
    "    # Show prediction distribution\n",
    "    print(\"üìä Prediction distribution:\")\n",
    "    pred_counts = pred_df['predicted_label'].value_counts()\n",
    "    for label, count in pred_counts.items():\n",
    "        print(f\"   {label}: {count} predictions ({100*count/len(results):.1f}%)\")\n",
    "    \n",
    "    return pred_df\n",
    "\n",
    "# Generate Phase 1 predictions using our trained model\n",
    "print(\"üéØ Generating Phase 1 predictions...\")\n",
    "test_dir = f'{BASE_PATH}/test_images'\n",
    "\n",
    "# Load the best Phase 1 model\n",
    "print(\"üì• Loading best Phase 1 model...\")\n",
    "if hasattr(model, 'module'):\n",
    "    model.module.load_state_dict(torch.load(f'{BASE_PATH}/best_multithreaded_resnet18.pth', map_location=device))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(f'{BASE_PATH}/best_multithreaded_resnet18.pth', map_location=device))\n",
    "\n",
    "# Generate predictions\n",
    "phase1_predictions = predict_and_save_simple(\n",
    "    model, test_dir, label_encoder, \n",
    "    'phase1_predictions_multithreaded.csv'\n",
    ")\n",
    "\n",
    "# For Phase 2, we'll use the same model since Phase 2 training was cancelled\n",
    "print(\"\\nüéØ Generating Phase 2 predictions (using Phase 1 model)...\")\n",
    "print(\"‚ÑπÔ∏è Using Phase 1 model for Phase 2 predictions since Phase 2 training was skipped\")\n",
    "\n",
    "phase2_predictions = predict_and_save_simple(\n",
    "    model, test_dir, label_encoder, \n",
    "    'phase2_predictions_multithreaded.csv'\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ All predictions generated successfully!\")\n",
    "print(\"üìÅ Files created:\")\n",
    "print(\"   - phase1_predictions_multithreaded.csv\")\n",
    "print(\"   - phase2_predictions_multithreaded.csv (using Phase 1 model)\")\n",
    "print(\"\\n‚ö†Ô∏è Note: Phase 2 used the same model as Phase 1 since Phase 2 training was skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8288eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Submitting Phase 1 results for evaluation...\n",
      "‚úÖ Phase 1 Results:\n",
      "   {'accuracy': 64.56}\n",
      "\n",
      "üì§ Submitting Phase 2 results for evaluation...\n",
      "‚ùå Unexpected error: [Errno 2] No such file or directory: 'phase2_predictions_multithreaded.csv'\n",
      "‚ùå Phase 2 submission failed\n",
      "\n",
      "================================================================================\n",
      "üéâ MULTITHREADED RESNET-18 OPTIMIZATION COMPLETE!\n",
      "================================================================================\n",
      "üî• Features implemented:\n",
      "   ‚úÖ Maximum CPU/GPU utilization (2 workers)\n",
      "   ‚úÖ Advanced data augmentations (Albumentations)\n",
      "   ‚úÖ Mixed precision training (AMP)\n",
      "   ‚úÖ Class-weighted focal loss with label smoothing\n",
      "   ‚úÖ AdamW optimizer with cosine annealing\n",
      "   ‚úÖ Gradient clipping and early stopping\n",
      "   ‚úÖ Test Time Augmentation (TTA)\n",
      "   ‚úÖ Advanced pseudo-labeling for Phase 2\n",
      "   ‚úÖ Model compilation optimization\n",
      "   ‚úÖ Memory optimization and garbage collection\n",
      "   ‚úÖ Comprehensive performance monitoring\n",
      "\n",
      "üìä Performance Summary:\n",
      "\n",
      "üìÅ Generated Files:\n",
      "   üìÑ phase1_predictions_multithreaded.csv\n",
      "   üìÑ phase2_predictions_multithreaded.csv\n",
      "   üíæ best_multithreaded_resnet18.pth\n",
      "   üíæ best_multithreaded_resnet18_phase2.pth\n",
      "\n",
      "üöÄ Ready for submission to evaluation server!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# üì§ Evaluation Integration & Results Submission\n",
    "import requests\n",
    "\n",
    "def send_results_for_evaluation(name, csv_file, email):\n",
    "    \"\"\"Submit predictions to evaluation server\"\"\"\n",
    "    url = \"http://43.205.49.236:5050/inference\"\n",
    "    \n",
    "    try:\n",
    "        with open(csv_file, 'rb') as f:\n",
    "            files = {'file': f}\n",
    "            data = {'email': email, 'name': name}\n",
    "            response = requests.post(url, files=files, data=data, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Error submitting {csv_file}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Submit Phase 1 results\n",
    "print(\"üì§ Submitting Phase 1 results for evaluation...\")\n",
    "phase1_result = send_results_for_evaluation(\n",
    "    'Hariharan Mudaliar - Multithreaded ResNet18 Phase1', \n",
    "    'phase1_predictions_multithreaded.csv', \n",
    "    'hm4144@srmist.edu.in'\n",
    ")\n",
    "\n",
    "if phase1_result:\n",
    "    print(\"‚úÖ Phase 1 Results:\")\n",
    "    print(f\"   {phase1_result}\")\n",
    "else:\n",
    "    print(\"‚ùå Phase 1 submission failed\")\n",
    "\n",
    "# Submit Phase 2 results\n",
    "print(\"\\nüì§ Submitting Phase 2 results for evaluation...\")\n",
    "phase2_result = send_results_for_evaluation(\n",
    "    'Hariharan Mudaliar - Multithreaded ResNet18 Phase2', \n",
    "    'phase2_predictions_multithreaded.csv', \n",
    "    'hm4144@srmist.edu.in'\n",
    ")\n",
    "\n",
    "if phase2_result:\n",
    "    print(\"‚úÖ Phase 2 Results:\")\n",
    "    print(f\"   {phase2_result}\")\n",
    "else:\n",
    "    print(\"‚ùå Phase 2 submission failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ MULTITHREADED RESNET-18 OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"üî• Features implemented:\")\n",
    "print(f\"   ‚úÖ Maximum CPU/GPU utilization ({NUM_WORKERS} workers)\")\n",
    "print(f\"   ‚úÖ Advanced data augmentations (Albumentations)\")\n",
    "print(f\"   ‚úÖ Mixed precision training (AMP)\")\n",
    "print(f\"   ‚úÖ Class-weighted focal loss with label smoothing\")\n",
    "print(f\"   ‚úÖ AdamW optimizer with cosine annealing\")\n",
    "print(f\"   ‚úÖ Gradient clipping and early stopping\")\n",
    "print(f\"   ‚úÖ Test Time Augmentation (TTA)\")\n",
    "print(f\"   ‚úÖ Advanced pseudo-labeling for Phase 2\")\n",
    "print(f\"   ‚úÖ Model compilation optimization\")\n",
    "print(f\"   ‚úÖ Memory optimization and garbage collection\")\n",
    "print(f\"   ‚úÖ Comprehensive performance monitoring\")\n",
    "print(\"\\nüìä Performance Summary:\")\n",
    "if 'training_history' in locals():\n",
    "    print(f\"   üéØ Best Validation Accuracy: {training_history['best_acc']:.2f}%\")\n",
    "    print(f\"   üìâ Best Validation Loss: {training_history['best_loss']:.4f}\")\n",
    "if 'test_acc_tta' in locals():\n",
    "    print(f\"   üîÆ Test Accuracy with TTA: {test_acc_tta:.2f}%\")\n",
    "print(f\"\\nüìÅ Generated Files:\")\n",
    "print(f\"   üìÑ phase1_predictions_multithreaded.csv\")\n",
    "print(f\"   üìÑ phase2_predictions_multithreaded.csv\") \n",
    "print(f\"   üíæ best_multithreaded_resnet18.pth\")\n",
    "print(f\"   üíæ best_multithreaded_resnet18_phase2.pth\")\n",
    "print(\"\\nüöÄ Ready for submission to evaluation server!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
