{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "168fc75d",
   "metadata": {},
   "source": [
    "# üöÄ Multithreaded ResNet-18 Optimizer\n",
    "\n",
    "**Maximum Performance Animal Classifier**\n",
    "\n",
    "This notebook implements a highly optimized ResNet-18 model with:\n",
    "- ‚ö° **Multithreading** for data loading and processing\n",
    "- üî• **Maximum CPU/GPU utilization**\n",
    "- üéØ **Best accuracy optimization techniques**\n",
    "- üìä **Phase 1 & Phase 2 submissions**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ef79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Environment Detection & Multithreading Setup\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "\n",
    "# Environment Detection\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"üåê Google Colab detected\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"üíª Local Jupyter detected\")\n",
    "\n",
    "# Base path configuration (adjusted for Multithreaded_approach folder)\n",
    "BASE_PATH = '/content' if IN_COLAB else '..'\n",
    "print(f\"üìÅ Base path: {BASE_PATH}\")\n",
    "\n",
    "# CPU/GPU Detection and Optimization\n",
    "cpu_count = mp.cpu_count()\n",
    "print(f\"üñ•Ô∏è  Available CPU cores: {cpu_count}\")\n",
    "\n",
    "# Set optimal number of workers\n",
    "NUM_WORKERS = min(cpu_count, 16)  # Cap at 16 to avoid memory issues\n",
    "print(f\"‚ö° Using {NUM_WORKERS} workers for data loading\")\n",
    "\n",
    "# Torch optimization settings\n",
    "torch.set_num_threads(cpu_count)\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üî• CUDA available - {torch.cuda.device_count()} GPUs\")\n",
    "    # Enable cudnn benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    print(\"üçé Metal Performance Shaders (MPS) available\")\n",
    "else:\n",
    "    print(\"üí™ Using optimized CPU with maximum threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b042160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Enhanced Package Installation with Performance Optimization\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def install_packages():\n",
    "    # Core packages with performance optimizations\n",
    "    pkgs = [\n",
    "        'torch', 'torchvision', 'torchaudio',  # Latest PyTorch with optimizations\n",
    "        'pandas', 'numpy', 'pillow', 'scikit-learn', \n",
    "        'tqdm', 'requests', 'matplotlib', 'seaborn',\n",
    "        'albumentations',  # Advanced augmentations\n",
    "        'timm',  # State-of-the-art models\n",
    "        'tensorboard',  # Training visualization\n",
    "    ]\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        pkgs.extend(['gdown', 'fastai'])  # Additional Colab optimizations\n",
    "    \n",
    "    print(\"üîß Installing optimized packages...\")\n",
    "    for pkg in pkgs:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, '-m', 'pip', 'install', pkg, '--upgrade'], \n",
    "                         check=True, capture_output=True)\n",
    "            print(f\"‚úÖ {pkg}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to install {pkg}: {e}\")\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# Set environment variables for maximum performance\n",
    "os.environ['OMP_NUM_THREADS'] = str(cpu_count)\n",
    "os.environ['MKL_NUM_THREADS'] = str(cpu_count)\n",
    "print(f\"üéØ Optimized for {cpu_count} CPU threads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1e37fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåê Advanced Data Download & Organization\n",
    "if IN_COLAB:\n",
    "    import gdown\n",
    "    print(\"üì• Downloading datasets in parallel...\")\n",
    "    \n",
    "    # Parallel download using threading\n",
    "    import threading\n",
    "    \n",
    "    def download_and_extract(url, filename, extract_to):\n",
    "        try:\n",
    "            gdown.download(url, f'{BASE_PATH}/{filename}', quiet=False)\n",
    "            os.system(f'cd {BASE_PATH} && unzip -q {filename}')\n",
    "            os.system(f'rm -rf {BASE_PATH}/__MACOSX')\n",
    "            os.system(f'mv {BASE_PATH}/{extract_to}/* {BASE_PATH}/')\n",
    "            os.system(f'rm -rf {BASE_PATH}/{extract_to} {BASE_PATH}/{filename}')\n",
    "            print(f\"‚úÖ {filename} processed\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {filename}: {e}\")\n",
    "    \n",
    "    # Download both datasets in parallel\n",
    "    thread1 = threading.Thread(target=download_and_extract, \n",
    "                              args=('https://drive.google.com/uc?id=18MA0qKg1rqP92HApr_Fjck7Zo4Bwdqdu', \n",
    "                                   'HV-AI-2025.zip', 'HV-AI-2025'))\n",
    "    thread2 = threading.Thread(target=download_and_extract,\n",
    "                              args=('https://drive.google.com/uc?id=1aszVlQFQOwJTy9tt79s7x87VJyYw-Sxy',\n",
    "                                   'HV-AI-2025-Test.zip', 'HV-AI-2025-Test'))\n",
    "    \n",
    "    thread1.start()\n",
    "    thread2.start()\n",
    "    thread1.join()\n",
    "    thread2.join()\n",
    "    \n",
    "    print(\"üéâ All datasets downloaded and organized!\")\n",
    "else:\n",
    "    print(\"üíª Assuming data is present in parent directory structure\")\n",
    "    print(f\"   - Labeled data: {BASE_PATH}/labeled_data/\")\n",
    "    print(f\"   - Unlabeled data: {BASE_PATH}/unlabeled_data/\")\n",
    "    print(f\"   - Test images: {BASE_PATH}/test_images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Enhanced Imports & Device Optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Enhanced data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter, ImageEnhance\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import timm\n",
    "\n",
    "# Analysis and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Device optimization with detailed info\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f'üöÄ Using CUDA - {gpu_name}')\n",
    "    print(f'üìä GPU Memory: {gpu_memory:.1f} GB')\n",
    "    # Enable mixed precision for faster training\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    scaler = GradScaler()\n",
    "    USE_AMP = True\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print('üçé Using Metal Performance Shaders (MPS)')\n",
    "    USE_AMP = False  # MPS doesn't support AMP yet\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('üí™ Using optimized CPU')\n",
    "    USE_AMP = False\n",
    "\n",
    "print(f\"‚ö° PyTorch version: {torch.__version__}\")\n",
    "print(f\"üéØ Device: {device}\")\n",
    "print(f\"üî• Mixed Precision: {USE_AMP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc75aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Advanced Data Analysis & Preprocessing\n",
    "print(\"üîç Loading and analyzing dataset...\")\n",
    "\n",
    "# Load data with enhanced analysis\n",
    "df = pd.read_csv(f'{BASE_PATH}/labeled_data/labeled_data.csv')\n",
    "print(f\"üìà Dataset shape: {df.shape}\")\n",
    "print(f\"üè∑Ô∏è  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Enhanced label encoding and analysis\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "print(f\"\\nüéØ Number of classes: {num_classes}\")\n",
    "print(\"üìä Class distribution:\")\n",
    "class_counts = df['label'].value_counts()\n",
    "for label, count in class_counts.items():\n",
    "    print(f\"   {label}: {count} samples\")\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "class_counts_array = np.bincount(df['encoded_label'])\n",
    "class_weights = 1.0 / class_counts_array\n",
    "class_weights = class_weights / class_weights.sum() * num_classes\n",
    "class_weights_tensor = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class weights calculated for balanced training\")\n",
    "print(\"üìà Data analysis complete!\")\n",
    "\n",
    "# Memory optimization\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
